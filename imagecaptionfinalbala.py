# -*- coding: utf-8 -*-
"""IMAGECAPTIONFINALbala.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rM2KcA3IwUUt4EEmqMY31CD4qg-EKxpg
"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, Concatenate, add
import tensorflow as tf

# Use same tokenizer and max_length as training
max_length = 59   # (7 is the same value you used earlier)
vocab_size = 24 # must match old tokenizer vocab

# --- Rebuild the architecture EXACTLY ---
inputs1 = Input(shape=(2048,))
fe1 = Dropout(0.5)(inputs1)
fe2 = Dense(512, activation='relu')(fe1)

    # Sequence model
inputs2 = Input(shape=(max_length,))
se1 = Embedding(vocab_size, 512, mask_zero=True)(inputs2)
se2 = Dropout(0.5)(se1)
se3 = LSTM(512)(se2)

    # Decoder model
decoder1 = add([fe2, se3])
decoder2 = Dense(512, activation='relu')(decoder1)
outputs = Dense(vocab_size, activation='softmax')(decoder2)

captionModel = Model(inputs=[inputs1, inputs2], outputs=outputs)

# --- Load weights ---
captionModel.load_weights("/content/Ex718CAPTIONMODEL.hdf5")
print("✅ Weights loaded successfully!")

import pickle
with open("/content/EX718TOKKENIZER.pkl", "rb") as f:
    tokenizer = pickle.load(f)

from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

def generate_caption(model, tokenizer, photo, max_length):
    '''
    Generates a caption for a given image feature vector.

    Args:
        model: The trained image captioning model.
        tokenizer: The tokenizer used for the captions.
        photo: The feature vector of the image.
        max_length: The maximum length of the caption.

    Returns:
        The generated caption string.
    '''
    in_text = 'startseq'
    for i in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length)
        yhat = model.predict([photo,sequence], verbose=0)
        yhat = np.argmax(yhat)
        word = tokenizer.index_word.get(yhat) # Use .get() for safer access
        if word is None:
            break
        in_text += ' ' + word
        if word == 'endseq':
            break
    return in_text

# Example usage (assuming you have an image feature vector 'image_feature_vector'):
# Replace 'image_feature_vector' with the actual feature vector of your image.
# caption = generate_caption(captionModel, tokenizer, image_feature_vector, max_length)
# print(caption)

from tensorflow.keras.applications.xception import Xception, preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array
import numpy as np

# Load the Xception model
feature_extraction_model = Xception(weights='imagenet', include_top=False, pooling='avg')
print("✅ Xception model loaded successfully!")

def extract_features(image_path, model):
    '''
    Extracts features from an image using a pre-trained model.

    Args:
        image_path: Path to the image file.
        model: The pre-trained feature extraction model (e.g., Xception).

    Returns:
        The feature vector of the image.
    '''
    # Load and preprocess the image
    img = load_img(image_path, target_size=(299, 299))
    img = img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = preprocess_input(img)

    # Get features
    features = model.predict(img, verbose=0)
    return features

"""Now you can use the `extract_features` function with your image path. For example, if your image is at `/content/Fungal diseases Saprolegniasis (118).jpg`:"""

# Replace with the path to your image
image_path = '/content/Fungal diseases Saprolegniasis (118).jpg'
image_feature_vector = extract_features(image_path, feature_extraction_model)
print("✅ Image features extracted successfully!")



caption = generate_caption(captionModel, tokenizer, image_feature_vector, max_length)
print(caption)

"""Now you have the `image_feature_vector` which you can use with the `generate_caption` function."""