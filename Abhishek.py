# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q1P9V73jr3oS4UX90mhb6w8PIR_hdfpR
"""

import os
import numpy as np
import pickle
import tensorflow as tf
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import (
    Input, Dense, Dropout, Embedding, LSTM,
    Concatenate, GlobalAveragePooling2D
)
from tensorflow.keras.applications import VGG19
from tensorflow.keras.applications.vgg19 import preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.metrics import classification_report

# ===================================================================
#  1. SETUP: CONFIGURE YOUR PATHS
#  (This setup assumes all files are in the same directory as the script)
# =================================C==================================

# --- Main Directory ---
BASE_DIR = "/content/files"

# --- Dataset Directories ---
ARCHIVE_FILE = os.path.join(BASE_DIR, "archive.zip")
# The 'val' directory will be created *inside* BASE_DIR after unzipping
VAL_DIR = os.path.join(BASE_DIR, "val")

# --- Sample Image Path (for testing) ---
# ! UPDATE THIS to the name of your sample image file in '/content/files'
SAMPLE_IMAGE_PATH = os.path.join(BASE_DIR, "NORMAL-2567-2.jpeg") # <-- CHANGE THIS

# --- File Names (all in BASE_DIR) ---
# Experiment 1 (Transfer Learning)
FILE_TRANSFER_WEIGHTS = os.path.join(BASE_DIR, 'vgg19_oct_transfer.weights.h5')

# Experiment 2 (Feature Extraction)
FILE_FEAT_WEIGHTS = os.path.join(BASE_DIR, 'vgg19_oct_feat_final.weights.h5')
FILE_VAL_FEATURES = os.path.join(BASE_DIR, 'features_val_vgg19.npy')
FILE_VAL_LABELS_NPY = os.path.join(BASE_DIR, 'labels_val_vgg19.npy')

# Experiment 3 (Image Captioning)
FILE_CAPTION_WEIGHTS = os.path.join(BASE_DIR, 'caption_vgg19_oct.weights.h5')
FILE_TOKENIZER = os.path.join(BASE_DIR, 'tokenizer_caption_vgg19_oct.pkl')

# --- Global Parameters ---
IMG_SIZE = (224, 224)
NUM_CLASSES = 4
CLASS_NAMES = ['CNV', 'DME', 'DRUSEN', 'NORMAL'] # Make sure this order is correct
VGG_FEATURE_DIM = 512
CAPTION_MAX_LENGTH = 7

!mv /content/files/dataset/val /content/files/val

def print_header(title):
    """Prints a formatted header."""
    print("\n" + "="*60)
    print(f"  {title.upper()}")
    print("="*60)

def unzip_data():
    """Unzips the archive file if VAL_DIR doesn't exist."""
    if os.path.isdir(VAL_DIR):
        print(f"'{VAL_DIR}' directory already exists. Skipping unzip.")
        return True

    print(f"'{VAL_DIR}' directory not found. Attempting to unzip '{ARCHIVE_FILE}'...")
    try:
        with zipfile.ZipFile(ARCHIVE_FILE, 'r') as zip_ref:
            zip_ref.extractall(BASE_DIR)
        print(f"✅ Successfully unzipped '{ARCHIVE_FILE}'.")

        # Handle nested folder structure (e.g., 'dataset/val')
        nested_val_path = os.path.join(BASE_DIR, "dataset", "val")
        if not os.path.isdir(VAL_DIR) and os.path.isdir(nested_val_path):
             print("Found nested 'dataset/val' structure. Moving 'val' up.")
             os.rename(nested_val_path, VAL_DIR)
             # Clean up
             os.rmdir(os.path.join(BASE_DIR, "dataset"))

        if not os.path.isdir(VAL_DIR):
             print(f"❌ ERROR: Unzipping did not create a '{VAL_DIR}' folder.")
             return False
        return True

    except FileNotFoundError:
        print(f"❌ FATAL ERROR: Cannot find '{ARCHIVE_FILE}'.")
        return False
    except Exception as e:
        print(f"❌ FATAL ERROR: Failed to unzip '{ARCHIVE_FILE}'. Error: {e}")
        return False
def load_vgg_base_model():
    """Loads the VGG19 feature extractor model."""
    try:
        print("\nLoading VGG19 base model (for feature extraction)...")
        model = VGG19(
            weights='imagenet',
            include_top=False,
            pooling='avg',
            input_shape=(*IMG_SIZE, 3)
        )
        model.trainable = False
        print("✅ VGG19 base model loaded.")
        return model
    except Exception as e:
        print(f"❌ FATAL ERROR: Failed to load VGG19 base model. Error: {e}")
        return None

def load_tokenizer():
    """Loads the captioning tokenizer."""
    try:
        with open(FILE_TOKENIZER, 'rb') as f:
            tokenizer = pickle.load(f)
        print("✅ Tokenizer loaded.")
        return tokenizer
    except FileNotFoundError:
        print(f"❌ FATAL ERROR: Tokenizer file not found at {FILE_TOKENIZER}")
        return None

def get_image_features(image_path, model):
    """Extracts VGG19 features from a single image path."""
    if model is None:
        raise ValueError("VGG19 Feature Extractor model is not loaded.")
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"Sample image not found at: {image_path}")
    img = load_img(image_path, target_size=IMG_SIZE)
    x = img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)
    return model.predict(x, verbose=0)

# ===================================================================
#  EXPERIMENT 1: TRANSFER LEARNING (Fine-Tuning)
# ===================================================================

def build_transfer_model():
    """Builds the VGG19 transfer learning architecture."""
    base = VGG19(weights=None, include_top=False, input_shape=(*IMG_SIZE, 3))
    x = base.output
    x = GlobalAveragePooling2D()(x)
    x = Dropout(0.5)(x)
    x = Dense(256, activation='relu')(x)
    outputs = Dense(NUM_CLASSES, activation='softmax')(x)
    return Model(inputs=base.input, outputs=outputs)

def load_transfer_model():
    """Loads and compiles the transfer learning model."""
    try:
        model = build_transfer_model()
        model.load_weights(FILE_TRANSFER_WEIGHTS)
        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
        print("✅ Transfer learning model built and weights loaded.")
        return model
    except Exception as e:
        print(f"❌ ERROR loading transfer model: {e}")
        return None

def test_single_image_transfer(model, image_path):
    """Tests the transfer learning model on one image."""
    print(f"\n--- Testing Transfer Model on: {image_path} ---")
    try:
        img = load_img(image_path, target_size=IMG_SIZE)
        img_array = img_to_array(img)
        img_array = np.expand_dims(img_array, axis=0)
        img_preprocessed = preprocess_input(img_array)

        prediction = model.predict(img_preprocessed, verbose=0)
        pred_index = np.argmax(prediction[0])
        pred_class = CLASS_NAMES[pred_index]
        confidence = prediction[0][pred_index] * 100
        print(f"Predicted Class: {pred_class} (Confidence: {confidence:.2f}%)")
    except Exception as e:
        print(f"❌ ERROR during single image test: {e}")

def generate_report_transfer(model, val_dir):
    """Generates the classification report for the transfer model."""
    print(f"\n--- Generating Transfer Model Report from: {val_dir} ---")
    try:
        val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
        val_generator = val_datagen.flow_from_directory(
            val_dir,
            target_size=IMG_SIZE,
            batch_size=32,
            class_mode='categorical',
            shuffle=False,
            classes=CLASS_NAMES
        )

        y_true = val_generator.classes
        y_pred_probs = model.predict(val_generator)
        y_pred = np.argmax(y_pred_probs, axis=1)

        print("\nClassification Report (Transfer Learning):")
        print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))
    except Exception as e:
        print(f"❌ ERROR during report generation: {e}")

# ===================================================================
#  EXPERIMENT 2: FEATURE EXTRACTION (Fixed VGG19)
# ===================================================================

def build_feature_model():
    """Builds the feature extraction classifier architecture."""
    return Sequential([
        Input(shape=(VGG_FEATURE_DIM,)),
        Dense(512, activation='relu'),
        Dropout(0.5),
        Dense(256, activation='relu'),
        Dropout(0.5),
        Dense(NUM_CLASSES, activation='softmax')
    ])

def load_feature_model():
    """Loads and compiles the feature extraction model."""
    try:
        model = build_feature_model()
        model.load_weights(FILE_FEAT_WEIGHTS)
        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
        print("✅ Feature extraction model built and weights loaded.")
        return model
    except Exception as e:
        print(f"❌ ERROR loading feature model: {e}")
        return None

def test_single_image_feature(model, image_path, vgg_extractor):
    """Tests the feature extraction model on one image."""
    print(f"\n--- Testing Feature Model on: {image_path} ---")
    try:
        features = get_image_features(image_path, vgg_extractor)
        prediction = model.predict(features, verbose=0)
        pred_index = np.argmax(prediction[0])
        pred_class = CLASS_NAMES[pred_index]
        confidence = prediction[0][pred_index] * 100
        print(f"Predicted Class: {pred_class} (Confidence: {confidence:.2f}%)")
    except Exception as e:
        print(f"❌ ERROR during single image test: {e}")

def generate_report_feature(model, val_dir, vgg_extractor):
    """Generates the classification report for the feature model."""
    print(f"\n--- Generating Feature Model Report ---")
    X_val, y_true = None, None

    try:
        if os.path.exists(FILE_VAL_FEATURES) and os.path.exists(FILE_VAL_LABELS_NPY):
            print("Loading pre-computed features from .npy files...")
            X_val = np.load(FILE_VAL_FEATURES)
            y_true = np.load(FILE_VAL_LABELS_NPY)
            print("✅ Features loaded from file.")
        else:
            print("Validation .npy files not found. Generating features on-the-fly...")
            val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
            val_generator = val_datagen.flow_from_directory(
                val_dir,
                target_size=IMG_SIZE,
                batch_size=32,
                class_mode='categorical',
                shuffle=False,
                classes=CLASS_NAMES
            )

            X_val = vgg_extractor.predict(
                val_generator,
                steps=np.ceil(val_generator.n / 32),
                verbose=1
            )
            y_true = val_generator.classes
            print("...Feature generation complete.")

            try:
                np.save(FILE_VAL_FEATURES, X_val)
                np.save(FILE_VAL_LABELS_NPY, y_true)
                print(f"✅ Features/labels saved to .npy files for next time.")
            except Exception as e:
                print(f"Warning: Could not save .npy files. {e}")

        y_pred_probs = model.predict(X_val)
        y_pred = np.argmax(y_pred_probs, axis=1)

        print("\nClassification Report (Feature Extraction):")
        print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))

    except Exception as e:
        print(f"❌ ERROR during report generation: {e}")

def build_captioning_model(vocab_size, max_length):
    """Builds the captioning model architecture."""
    inputs1 = Input(shape=(VGG_FEATURE_DIM,))
    fe1 = Dropout(0.5)(inputs1)
    fe2 = Dense(256, activation='relu')(fe1)
    inputs2 = Input(shape=(max_length,))
    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
    se2 = LSTM(256)(se1)
    decoder1 = Concatenate()([fe2, se2])
    decoder2 = Dense(256, activation='relu')(decoder1)
    outputs = Dense(vocab_size, activation='softmax')(decoder2)
    return Model(inputs=[inputs1, inputs2], outputs=outputs)

def idx_to_word(integer, tokenizer):
    for word, index in tokenizer.word_index.items():
        if index == integer:
            return word
    return None

def generate_caption(model, tokenizer, photo_features, max_length):
    """Generates a caption for one image's features."""
    in_text = '<start>'
    for _ in range(max_length):
        seq = tokenizer.texts_to_sequences([in_text])[0]
        seq = pad_sequences([seq], maxlen=max_length, padding='post')
        yhat = model.predict([photo_features, seq], verbose=0)
        yhat = np.argmax(yhat)
        word = idx_to_word(yhat, tokenizer)
        if word is None: break
        in_text += ' ' + word
        if word == '<end>': break
    return in_text.replace('<start> ', '').replace(' <end>', '')

def load_caption_model(tokenizer):
    """Loads and compiles the captioning model."""
    if tokenizer is None:
        print("❌ Cannot load captioning model without a tokenizer.")
        return None
    try:
        vocab_size = len(tokenizer.word_index) + 1
        model = build_captioning_model(vocab_size, CAPTION_MAX_LENGTH)
        model.load_weights(FILE_CAPTION_WEIGHTS)
        model.compile(loss='categorical_crossentropy', optimizer='adam')
        print("✅ Captioning model built and weights loaded.")
        return model
    except Exception as e:
        print(f"❌ ERROR loading captioning model: {e}")
        return None

def test_single_image_caption(model, tokenizer, image_path, vgg_extractor):
    """Tests the captioning model on one image."""
    print(f"\n--- Testing Captioning Model on: {image_path} ---")
    try:
        features = get_image_features(image_path, vgg_extractor)
        caption = generate_caption(model, tokenizer, features, CAPTION_MAX_LENGTH)
        print(f"Predicted Caption: {caption}")
    except Exception as e:
        print(f"❌ ERROR during single image test: {e}")

def generate_report_caption(model):
    """Prints the 'Not Applicable' message for captioning."""
    print(f"\n--- Generating Captioning Model Report ---")
    print("Not Applicable: Classification reports are not used for captioning models.")
    print("Metrics like BLEU, ROUGE, or METEOR are used instead.")

if __name__ == "__main__":
    if not os.path.isdir(BASE_DIR):
        print(f"❌ FATAL ERROR: Base directory '{BASE_DIR}' does not exist.")
        sys.exit(1)

    # --- 1. Run Setup Tasks ---
    if not unzip_data():
        sys.exit(1) # Stop if data is not available

    vgg_feature_extractor = load_vgg_base_model()
    tokenizer = load_tokenizer()

    if vgg_feature_extractor is None or tokenizer is None:
        print("❌ FATAL ERROR: Missing a critical component (VGG model or tokenizer).")
        sys.exit(1)

    if not os.path.isfile(SAMPLE_IMAGE_PATH):
        print(f"❌ WARNING: Sample image not found at '{SAMPLE_IMAGE_PATH}'.")
        print("     Single image tests will fail. Please update the path.")

    # --- 2. Run Experiment 1 ---
    print_header("EXPERIMENT 1: TRANSFER LEARNING")
    transfer_model = load_transfer_model()
    if transfer_model:
        test_single_image_transfer(transfer_model, SAMPLE_IMAGE_PATH)
        #generate_report_transfer(transfer_model, VAL_DIR)

    # --- 3. Run Experiment 2 ---
    print_header("EXPERIMENT 2: FEATURE EXTRACTION")
    feature_model = load_feature_model()
    if feature_model:
        test_single_image_feature(feature_model, SAMPLE_IMAGE_PATH, vgg_feature_extractor)
        #generate_report_feature(feature_model, VAL_DIR, vgg_feature_extractor)

    # --- 4. Run Experiment 3 ---
    print_header("EXPERIMENT 3: IMAGE CAPTIONING")
    caption_model = load_caption_model(tokenizer)
    if caption_model:
        test_single_image_caption(caption_model, tokenizer, SAMPLE_IMAGE_PATH, vgg_feature_extractor)
        #generate_report_caption(caption_model)

    print("\n" + "="*60)
    print("  ALL EXPERIMENTS FINISHED")
    print("="*60)